{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/03 22:04:11 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName('hw-5') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.3.2'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = 'fhvhv_tripdata_2021-06.csv.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: URL=https://github.com/DataTalksClub/nyc-tlc-data/releases/download\n"
     ]
    }
   ],
   "source": [
    "%env URL=https://github.com/DataTalksClub/nyc-tlc-data/releases/download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://github.com/DataTalksClub/nyc-tlc-data/releases/download/fhvhv/fhvhv_tripdata_2021-06.csv.gz\n"
     ]
    }
   ],
   "source": [
    "!echo $$URL/fhvhv/{data_file}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/duy.ton/CodeSpace/workshop/dezc_230/wk_5_batch_processing\n",
      "mkdir: data: File exists\n",
      "total 8\n",
      "drwxr-xr-x  2 duy.ton  staff    64 Mar  3 22:55 \u001b[1m\u001b[36mdata\u001b[m\u001b[m\n",
      "-rw-r--r--  1 duy.ton  staff  3312 Mar  3 22:55 hw.ipynb\n"
     ]
    }
   ],
   "source": [
    "!pwd\n",
    "!mkdir data\n",
    "!ls -lH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100  167M  100  167M    0     0   917k      0  0:03:07  0:03:07 --:--:-- 1030k:03:18 1097k10  167M   10 16.8M    0     0   840k      0  0:03:24  0:00:20  0:03:04  720k  0  1003k      0  0:02:51  0:00:36  0:02:15 1189k0   974k      0  0:02:56  0:00:41  0:02:15  762k  0  0:03:00  0:01:15  0:01:45 1130kk      0  0:03:01  0:01:24  0:01:37 1050k 0   945k      0  0:03:01  0:01:47  0:01:14  604k  0  0:03:03  0:01:59  0:01:04  835k  0     0   921k      0  0:03:06  0:02:25  0:00:41  772k\n"
     ]
    }
   ],
   "source": [
    "!curl -L $$URL/fhvhv/{data_file} -o data/{data_file}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 360456\n",
      "-rw-r--r--  1 duy.ton  staff  175799316 Mar  3 23:00 fhvhv_tripdata_2021-06.csv.gz\n"
     ]
    }
   ],
   "source": [
    "!ls -lH data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.types as Tp\n",
    "import pyspark.sql.functions as Fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "fhvhv_schema = Tp.StructType([\n",
    "    Tp.StructField('dispatching_base_num', Tp.StringType(), True),\n",
    "    Tp.StructField('pickup_datetime', Tp.TimestampType(), True),\n",
    "    Tp.StructField('dropoff_datetime', Tp.TimestampType(), True),\n",
    "    Tp.StructField('PULocationID', Tp.IntegerType(), True),\n",
    "    Tp.StructField('DOLocationID', Tp.IntegerType(), True),\n",
    "    Tp.StructField('SR_Flag', Tp.StringType(), True),\n",
    "    Tp.StructField('affiliated_base_number', Tp.StringType(), True),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.schema(fhvhv_schema).csv(f'data/{data_file}', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dispatching_base_num: string (nullable = true)\n",
      " |-- pickup_datetime: timestamp (nullable = true)\n",
      " |-- dropoff_datetime: timestamp (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      " |-- SR_Flag: string (nullable = true)\n",
      " |-- affiliated_base_number: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+\n",
      "|dispatching_base_num|    pickup_datetime|   dropoff_datetime|PULocationID|DOLocationID|SR_Flag|affiliated_base_number|\n",
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+\n",
      "|              B02764|2021-06-01 00:02:41|2021-06-01 00:07:46|         174|          18|      N|                B02764|\n",
      "|              B02764|2021-06-01 00:16:16|2021-06-01 00:21:14|          32|         254|      N|                B02764|\n",
      "|              B02764|2021-06-01 00:27:01|2021-06-01 00:42:11|         240|         127|      N|                B02764|\n",
      "|              B02764|2021-06-01 00:46:08|2021-06-01 00:53:45|         127|         235|      N|                B02764|\n",
      "|              B02510|2021-06-01 00:45:42|2021-06-01 01:03:33|         144|         146|      N|                  null|\n",
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/03 23:10:31 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_partitioned = df.repartition(12)\n",
    "df_partitioned.write.parquet(f'data/12-partitions/', mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 591360\n",
      "-rw-r--r--  1 duy.ton  staff     0B Mar  3 23:10 _SUCCESS\n",
      "-rw-r--r--  1 duy.ton  staff    24M Mar  3 23:10 part-00000-d6aad2ba-7152-48d6-9515-3d3bf6a6ae1b-c000.snappy.parquet\n",
      "-rw-r--r--  1 duy.ton  staff    24M Mar  3 23:10 part-00001-d6aad2ba-7152-48d6-9515-3d3bf6a6ae1b-c000.snappy.parquet\n",
      "-rw-r--r--  1 duy.ton  staff    24M Mar  3 23:10 part-00002-d6aad2ba-7152-48d6-9515-3d3bf6a6ae1b-c000.snappy.parquet\n",
      "-rw-r--r--  1 duy.ton  staff    24M Mar  3 23:10 part-00003-d6aad2ba-7152-48d6-9515-3d3bf6a6ae1b-c000.snappy.parquet\n",
      "-rw-r--r--  1 duy.ton  staff    24M Mar  3 23:10 part-00004-d6aad2ba-7152-48d6-9515-3d3bf6a6ae1b-c000.snappy.parquet\n",
      "-rw-r--r--  1 duy.ton  staff    24M Mar  3 23:10 part-00005-d6aad2ba-7152-48d6-9515-3d3bf6a6ae1b-c000.snappy.parquet\n",
      "-rw-r--r--  1 duy.ton  staff    24M Mar  3 23:10 part-00006-d6aad2ba-7152-48d6-9515-3d3bf6a6ae1b-c000.snappy.parquet\n",
      "-rw-r--r--  1 duy.ton  staff    24M Mar  3 23:10 part-00007-d6aad2ba-7152-48d6-9515-3d3bf6a6ae1b-c000.snappy.parquet\n",
      "-rw-r--r--  1 duy.ton  staff    24M Mar  3 23:10 part-00008-d6aad2ba-7152-48d6-9515-3d3bf6a6ae1b-c000.snappy.parquet\n",
      "-rw-r--r--  1 duy.ton  staff    24M Mar  3 23:10 part-00009-d6aad2ba-7152-48d6-9515-3d3bf6a6ae1b-c000.snappy.parquet\n",
      "-rw-r--r--  1 duy.ton  staff    24M Mar  3 23:10 part-00010-d6aad2ba-7152-48d6-9515-3d3bf6a6ae1b-c000.snappy.parquet\n",
      "-rw-r--r--  1 duy.ton  staff    24M Mar  3 23:10 part-00011-d6aad2ba-7152-48d6-9515-3d3bf6a6ae1b-c000.snappy.parquet\n"
     ]
    }
   ],
   "source": [
    "!ls -lh data/12-partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "452470"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df_jun_15 = df.filter(Fn.to_date(df.pickup_datetime) == '2021-06-15')\n",
    "# a more elaborated version of above\n",
    "col_pickup_dt = Fn.col('pickup_datetime')\n",
    "df_jun_15 = df \\\n",
    "    .filter(Fn.month(col_pickup_dt) == 6) \\\n",
    "    .filter(Fn.dayofmonth(col_pickup_dt) == 15)\n",
    "\n",
    "df_jun_15.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('dispatching_base_num', 'string'),\n",
       " ('pickup_datetime', 'timestamp'),\n",
       " ('dropoff_datetime', 'timestamp'),\n",
       " ('PULocationID', 'int'),\n",
       " ('DOLocationID', 'int'),\n",
       " ('SR_Flag', 'string'),\n",
       " ('affiliated_base_number', 'string')]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 48:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------+------------+-----------+\n",
      "|longest                            |longest_secs|longest_hrs|\n",
      "+-----------------------------------+------------+-----------+\n",
      "|INTERVAL '2 18:52:44' DAY TO SECOND|240764      |66.88      |\n",
      "+-----------------------------------+------------+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "calc_trip_duration = df.dropoff_datetime - df.pickup_datetime\n",
    "\n",
    "df_max_trip = df \\\n",
    "    .withColumn('trip_duration', calc_trip_duration) \\\n",
    "    .select(Fn.max('trip_duration').alias('longest'))\n",
    "\n",
    "df_max_trip \\\n",
    "    .withColumn('longest_secs', Fn.col('longest').cast('long')) \\\n",
    "    .withColumn('longest_hrs', Fn.round(Fn.col('longest_secs') / 3600, 2)) \\\n",
    "    .show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_zones = 'taxi_zone_lookup.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100 12322  100 12322    0     0      0      0 --:--:-- --:--:-- --:--:--     00     0  10366      0  0:00:01  0:00:01 --:--:-- 31757\n"
     ]
    }
   ],
   "source": [
    "!curl -L $$URL/misc/{data_zones} -o data/{data_zones}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_zones = spark.read.csv(f'data/{data_zones}', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+--------------------+------------+\n",
      "|LocationID|      Borough|                Zone|service_zone|\n",
      "+----------+-------------+--------------------+------------+\n",
      "|         1|          EWR|      Newark Airport|         EWR|\n",
      "|         2|       Queens|         Jamaica Bay|   Boro Zone|\n",
      "|         3|        Bronx|Allerton/Pelham G...|   Boro Zone|\n",
      "|         4|    Manhattan|       Alphabet City| Yellow Zone|\n",
      "|         5|Staten Island|       Arden Heights|   Boro Zone|\n",
      "+----------+-------------+--------------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_zones.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_joined = df.join(df_zones, on=[df.PULocationID == df_zones.LocationID])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 58:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+\n",
      "|pickup_zone         |count |\n",
      "+--------------------+------+\n",
      "|Crown Heights North |231279|\n",
      "|East Village        |221244|\n",
      "|JFK Airport         |188867|\n",
      "|Bushwick South      |187929|\n",
      "|East New York       |186780|\n",
      "|TriBeCa/Civic Center|164344|\n",
      "|LaGuardia Airport   |161596|\n",
      "|Union Sq            |158937|\n",
      "|West Village        |154698|\n",
      "|Astoria             |152493|\n",
      "+--------------------+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_joined \\\n",
    "    .withColumnRenamed('Zone', 'pickup_zone') \\\n",
    "    .groupBy('pickup_zone').count() \\\n",
    "    .orderBy('count', ascending=False) \\\n",
    "    .limit(10) \\\n",
    "    .show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dez_x86",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
